"""
Text Splitter for RAG Pipeline
ë¬¸ì„œë¥¼ ì˜ë¯¸ ë‹¨ìœ„(ì„¹ì…˜)ë¡œ ë¶„í• í•˜ê³  ì²­í‚¹í•˜ëŠ” ëª¨ë“ˆ

í•µì‹¬ ì „ëµ:
1. ì„¹ì…˜ í—¤ë” ê°ì§€ (ì´ë ¥ì„œ/JD ê°ê°ì˜ íŒ¨í„´)
2. ì„¹ì…˜ë³„ ë¶„ë¦¬ í›„ ê°œë³„ ì²­í‚¹
3. ì²­í¬ì— ì„¹ì…˜ íƒ€ì… ë©”íƒ€ë°ì´í„° ì¶”ê°€
"""

import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

from app.models.schemas import FileTypeEnum


# ============================================================================
# ì„¹ì…˜ íƒ€ì… ì •ì˜
# ============================================================================


class SectionType(str, Enum):
    """ë¬¸ì„œ ì„¹ì…˜ íƒ€ì…"""

    # ì´ë ¥ì„œ ì„¹ì…˜
    SUMMARY = "summary"
    EXPERIENCE = "experience"
    SKILLS = "skills"
    EDUCATION = "education"
    PROJECTS = "projects"
    CERTIFICATIONS = "certifications"
    AWARDS = "awards"
    CONTACT = "contact"

    # JD ì„¹ì…˜
    COMPANY_INFO = "company_info"
    RESPONSIBILITIES = "responsibilities"
    REQUIREMENTS = "requirements"
    PREFERRED = "preferred"
    BENEFITS = "benefits"
    SALARY = "salary"
    TECH_STACK = "tech_stack"

    # ê³µí†µ
    UNKNOWN = "unknown"


# ============================================================================
# ì„¹ì…˜ í—¤ë” íŒ¨í„´ ì •ì˜ (í•œêµ­ì–´/ì˜ì–´) - ê°•í™”ëœ ë²„ì „
# ============================================================================

# í—¤ë” ì•ì— ì˜¬ ìˆ˜ ìˆëŠ” prefix (ì´ëª¨ì§€, ë²ˆí˜¸, ê¸°í˜¸ ë“±)
HEADER_PREFIX = r"^[\s]*(?:[\d\.\-\*\â€¢\â—¦\â–ª\â–¸\â–º\â¤\âœ“\âœ”\â˜‘\ğŸ”¹\ğŸ”¸\ğŸ“Œ\ğŸ“\ğŸ’¼\ğŸ¯\âœ¨\â­\â˜…\â˜†\#\[\]ã€ã€‘\(\)]*\s*)?"

# ì´ë ¥ì„œ ì„¹ì…˜ íŒ¨í„´ (ë” ìœ ì—°í•œ ë§¤ì¹­)
RESUME_SECTION_PATTERNS: Dict[SectionType, List[str]] = {
    SectionType.SUMMARY: [
        # ìê¸°ì†Œê°œ ê´€ë ¨
        HEADER_PREFIX + r"(?:ìê¸°\s*ì†Œê°œ|ìê¸°ì†Œê°œì„œ?|ì†Œê°œ|ìì†Œì„œ)",
        HEADER_PREFIX + r"(?:about\s*me|summary|profile|introduction)",
        HEADER_PREFIX + r"(?:professional\s*summary|career\s*summary|executive\s*summary)",
        HEADER_PREFIX + r"(?:ê°„ëµ\s*ì†Œê°œ|ì¸ì‚¬ë§|ì»¤ë¦¬ì–´\s*ìš”ì•½)",
    ],
    SectionType.EXPERIENCE: [
        # ê²½ë ¥ ê´€ë ¨
        HEADER_PREFIX + r"(?:ê²½ë ¥\s*ì‚¬í•­|ê²½ë ¥ì‚¬í•­|ê²½ë ¥|ì´ë ¥|ì»¤ë¦¬ì–´)",
        HEADER_PREFIX + r"(?:ì—…ë¬´\s*ê²½ë ¥|ê·¼ë¬´\s*ê²½ë ¥|ì§ì¥\s*ê²½ë ¥|íšŒì‚¬\s*ê²½ë ¥)",
        HEADER_PREFIX + r"(?:work\s*experience|experience|employment|career)",
        HEADER_PREFIX + r"(?:professional\s*experience|work\s*history)",
        HEADER_PREFIX + r"(?:ê²½í—˜|ì‹¤ë¬´\s*ê²½í—˜|í”„ë¡œì íŠ¸\s*ê²½í—˜)",
    ],
    SectionType.SKILLS: [
        # ê¸°ìˆ  ìŠ¤íƒ ê´€ë ¨
        HEADER_PREFIX + r"(?:ê¸°ìˆ \s*ìŠ¤íƒ|ê¸°ìˆ ìŠ¤íƒ|ìŠ¤í‚¬|ê¸°ìˆ |ì—­ëŸ‰)",
        HEADER_PREFIX + r"(?:ë³´ìœ \s*ê¸°ìˆ |í•µì‹¬\s*ì—­ëŸ‰|ì „ë¬¸\s*ê¸°ìˆ |ê¸°ìˆ \s*ì—­ëŸ‰)",
        HEADER_PREFIX + r"(?:skills?|technical\s*skills?|tech\s*stack)",
        HEADER_PREFIX + r"(?:core\s*competencies|expertise|technologies|tools)",
        HEADER_PREFIX + r"(?:ì‚¬ìš©\s*ê¸°ìˆ |ê°œë°œ\s*í™˜ê²½|ê°œë°œ\s*ìŠ¤íƒ)",
    ],
    SectionType.EDUCATION: [
        # í•™ë ¥ ê´€ë ¨
        HEADER_PREFIX + r"(?:í•™ë ¥\s*ì‚¬í•­|í•™ë ¥ì‚¬í•­|í•™ë ¥|êµìœ¡)",
        HEADER_PREFIX + r"(?:education|academic|educational\s*background)",
        HEADER_PREFIX + r"(?:í•™êµ|ëŒ€í•™|ì¡¸ì—…)",
    ],
    SectionType.PROJECTS: [
        # í”„ë¡œì íŠ¸ ê´€ë ¨
        HEADER_PREFIX + r"(?:í”„ë¡œì íŠ¸|í¬íŠ¸í´ë¦¬ì˜¤|portfolio|projects?)",
        HEADER_PREFIX + r"(?:ê°œì¸\s*í”„ë¡œì íŠ¸|ì‚¬ì´ë“œ\s*í”„ë¡œì íŠ¸|íŒ€\s*í”„ë¡œì íŠ¸)",
        HEADER_PREFIX + r"(?:personal\s*projects?|side\s*projects?)",
        HEADER_PREFIX + r"(?:ì£¼ìš”\s*í”„ë¡œì íŠ¸|ëŒ€í‘œ\s*í”„ë¡œì íŠ¸)",
    ],
    SectionType.CERTIFICATIONS: [
        # ìê²©ì¦ ê´€ë ¨
        HEADER_PREFIX + r"(?:ìê²©ì¦|ìê²©\s*ì‚¬í•­|ìê²©ì‚¬í•­|ë©´í—ˆ|ë¼ì´ì„ ìŠ¤)",
        HEADER_PREFIX + r"(?:certifications?|licenses?|certificates?)",
        HEADER_PREFIX + r"(?:professional\s*certifications?)",
    ],
    SectionType.AWARDS: [
        # ìˆ˜ìƒ ê´€ë ¨
        HEADER_PREFIX + r"(?:ìˆ˜ìƒ\s*ê²½ë ¥|ìˆ˜ìƒê²½ë ¥|ìˆ˜ìƒ|ì„±ê³¼|ì—…ì )",
        HEADER_PREFIX + r"(?:awards?|honors?|achievements?)",
    ],
    SectionType.CONTACT: [
        # ì—°ë½ì²˜ ê´€ë ¨
        HEADER_PREFIX + r"(?:ì—°ë½ì²˜|ì¸ì \s*ì‚¬í•­|ê°œì¸\s*ì •ë³´|contact)",
        HEADER_PREFIX + r"(?:ê¸°ë³¸\s*ì •ë³´|ì¸ì ì‚¬í•­)",
    ],
}

# JD ì„¹ì…˜ íŒ¨í„´ (ë” ìœ ì—°í•œ ë§¤ì¹­)
JD_SECTION_PATTERNS: Dict[SectionType, List[str]] = {
    SectionType.COMPANY_INFO: [
        # íšŒì‚¬ ì†Œê°œ
        HEADER_PREFIX + r"(?:íšŒì‚¬\s*ì†Œê°œ|ê¸°ì—…\s*ì†Œê°œ|íšŒì‚¬ì†Œê°œ|ê¸°ì—…ì†Œê°œ)",
        HEADER_PREFIX + r"(?:ìš°ë¦¬\s*íšŒì‚¬|about\s*us|company|our\s*company)",
        HEADER_PREFIX + r"(?:who\s*we\s*are|íšŒì‚¬\s*ì •ë³´|ê¸°ì—…\s*ì •ë³´)",
        HEADER_PREFIX + r"(?:ì¡°ì§\s*ì†Œê°œ|íŒ€\s*ì†Œê°œ)",
    ],
    SectionType.RESPONSIBILITIES: [
        # ì£¼ìš” ì—…ë¬´ / ë‹´ë‹¹ ì—…ë¬´
        HEADER_PREFIX + r"(?:ì£¼ìš”\s*ì—…ë¬´|ì£¼ìš”ì—…ë¬´|ë‹´ë‹¹\s*ì—…ë¬´|ë‹´ë‹¹ì—…ë¬´)",
        HEADER_PREFIX + r"(?:ì—…ë¬´\s*ë‚´ìš©|ì—…ë¬´ë‚´ìš©|í•˜ëŠ”\s*ì¼|ì—­í• )",
        HEADER_PREFIX + r"(?:responsibilities|duties|role|what\s*you.+do)",
        HEADER_PREFIX + r"(?:job\s*description|key\s*responsibilities)",
        HEADER_PREFIX + r"(?:ì—…ë¬´\s*ì†Œê°œ|ì´ëŸ°\s*ì¼)",
        HEADER_PREFIX + r"(?:ì£¼ìš”\s*ì—­í• |ë‹´ë‹¹\s*ì—­í• )",
    ],
    SectionType.REQUIREMENTS: [
        # ìê²© ìš”ê±´ / í•„ìˆ˜ ì¡°ê±´
        HEADER_PREFIX + r"(?:ìê²©\s*ìš”ê±´|ìê²©ìš”ê±´|í•„ìˆ˜\s*ìš”ê±´|í•„ìˆ˜ìš”ê±´)",
        HEADER_PREFIX + r"(?:ì§€ì›\s*ìê²©|ì§€ì›ìê²©|í•„ìˆ˜\s*ì¡°ê±´|í•„ìˆ˜ì¡°ê±´)",
        HEADER_PREFIX + r"(?:ê¸°ë³¸\s*ìê²©|ìê²©\s*ì¡°ê±´|ìê²©ì¡°ê±´)",
        HEADER_PREFIX + r"(?:requirements?|qualifications?|required)",
        HEADER_PREFIX + r"(?:must\s*have|minimum\s*requirements?)",
        HEADER_PREFIX + r"(?:ì´ëŸ°\s*ë¶„.*ì°¾|í•„ìš”\s*ì—­ëŸ‰|í•„ìˆ˜\s*ì—­ëŸ‰)",
        HEADER_PREFIX + r"(?:ì§€ì›\s*ìš”ê±´|ì±„ìš©\s*ì¡°ê±´)",
    ],
    SectionType.PREFERRED: [
        # ìš°ëŒ€ ì‚¬í•­
        HEADER_PREFIX + r"(?:ìš°ëŒ€\s*ì‚¬í•­|ìš°ëŒ€ì‚¬í•­|ìš°ëŒ€\s*ì¡°ê±´|ìš°ëŒ€ì¡°ê±´)",
        HEADER_PREFIX + r"(?:ê°€ì \s*ì‚¬í•­|ê°€ì ì‚¬í•­|í”ŒëŸ¬ìŠ¤|plus)",
        HEADER_PREFIX + r"(?:preferred|nice\s*to\s*have|bonus|desired)",
        HEADER_PREFIX + r"(?:ì„ í˜¸\s*ì‚¬í•­|ì´ëŸ°\s*ë¶„.*ìš°ëŒ€|ìš°ëŒ€\s*ì—­ëŸ‰)",
        HEADER_PREFIX + r"(?:ì¶”ê°€\s*ìš°ëŒ€|ê²½í—˜.*ìˆìœ¼ë©´)",
    ],
    SectionType.BENEFITS: [
        # ë³µë¦¬í›„ìƒ
        HEADER_PREFIX + r"(?:ë³µë¦¬\s*í›„ìƒ|ë³µë¦¬í›„ìƒ|ë³µì§€|í˜œíƒ|ë² ë„¤í•)",
        HEADER_PREFIX + r"(?:benefits?|perks?|what\s*we\s*offer)",
        HEADER_PREFIX + r"(?:ê·¼ë¬´\s*í™˜ê²½|ê·¼ë¬´í™˜ê²½|ìš°ë¦¬ê°€\s*ì œê³µ)",
        HEADER_PREFIX + r"(?:ì§€ì›\s*ì‚¬í•­|ê·¼ë¬´\s*ì¡°ê±´|ì²˜ìš°)",
    ],
    SectionType.SALARY: [
        # ê¸‰ì—¬
        HEADER_PREFIX + r"(?:ê¸‰ì—¬|ì—°ë´‰|ë³´ìƒ|ì²˜ìš°|salary|compensation)",
        HEADER_PREFIX + r"(?:pay|remuneration|ê¸‰ì—¬\s*ì¡°ê±´)",
    ],
    SectionType.TECH_STACK: [
        # ê¸°ìˆ  ìŠ¤íƒ (JDìš©)
        HEADER_PREFIX + r"(?:ê¸°ìˆ \s*ìŠ¤íƒ|ê¸°ìˆ ìŠ¤íƒ|tech\s*stack)",
        HEADER_PREFIX + r"(?:ê°œë°œ\s*í™˜ê²½|ì‚¬ìš©\s*ê¸°ìˆ |ê¸°ìˆ \s*í™˜ê²½)",
        HEADER_PREFIX + r"(?:tools?|technologies|ìŠ¤íƒ)",
    ],
}


# ============================================================================
# í‚¤ì›Œë“œ ê¸°ë°˜ ì„¹ì…˜ ì¶”ë¡  (í—¤ë”ê°€ ì—†ëŠ” ê²½ìš° ë‚´ìš© ê¸°ë°˜ ì¶”ë¡ )
# ============================================================================

SECTION_KEYWORDS: Dict[SectionType, List[str]] = {
    SectionType.SKILLS: [
        "react", "vue", "angular", "javascript", "typescript", "python", "java",
        "node", "spring", "django", "fastapi", "docker", "kubernetes", "aws",
        "git", "mysql", "postgresql", "mongodb", "redis", "linux", "ci/cd",
        "html", "css", "sass", "webpack", "jest", "graphql", "rest", "api",
    ],
    SectionType.REQUIREMENTS: [
        "ë…„ ì´ìƒ", "ê²½ë ¥", "í•„ìˆ˜", "í•™ì‚¬", "ì„ì‚¬", "í•™ìœ„", "ì „ê³µ",
        "years of experience", "required", "degree", "bachelor", "master",
    ],
    SectionType.PREFERRED: [
        "ìš°ëŒ€", "ìˆìœ¼ë©´", "ê²½í—˜ì", "ê°€ì ", "preferred", "plus", "nice to have",
    ],
    SectionType.EXPERIENCE: [
        "ì¬ì§", "ê·¼ë¬´", "ë‹´ë‹¹", "ê°œë°œ", "ìš´ì˜", "ê¸°ì—¬", "ë‹¬ì„±", "ë¦¬ë“œ",
    ],
}


def infer_section_from_content(content: str) -> Optional[SectionType]:
    """
    ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ íƒ€ì… ì¶”ë¡ 

    Args:
        content: ì²­í¬ ë‚´ìš©

    Returns:
        ì¶”ë¡ ëœ ì„¹ì…˜ íƒ€ì… ë˜ëŠ” None
    """
    content_lower = content.lower()

    # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
    scores: Dict[SectionType, int] = {}

    for section_type, keywords in SECTION_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw.lower() in content_lower)
        if score > 0:
            scores[section_type] = score

    if scores:
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì„¹ì…˜ ë°˜í™˜
        return max(scores, key=scores.get)

    return None


# ============================================================================
# ì„¹ì…˜ ê°ì§€ í—¬í¼ í•¨ìˆ˜
# ============================================================================


def detect_section_type(
    line: str,
    file_type: FileTypeEnum,
) -> Optional[SectionType]:
    """
    í…ìŠ¤íŠ¸ ë¼ì¸ì—ì„œ ì„¹ì…˜ í—¤ë”ë¥¼ ê°ì§€

    Args:
        line: ê²€ì‚¬í•  í…ìŠ¤íŠ¸ ë¼ì¸
        file_type: íŒŒì¼ íƒ€ì… (resume/job_description)

    Returns:
        ê°ì§€ëœ ì„¹ì…˜ íƒ€ì… ë˜ëŠ” None
    """
    # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ íŒ¨í„´ ì„ íƒ
    patterns = (
        RESUME_SECTION_PATTERNS
        if file_type == FileTypeEnum.RESUME
        else JD_SECTION_PATTERNS
    )

    # ë¼ì¸ ì •ê·œí™” (ì•ë’¤ ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ìëŠ” ìœ ì§€)
    cleaned_line = line.strip()

    # ë¹ˆ ë¼ì¸ì´ê±°ë‚˜ ë„ˆë¬´ ê¸´ ë¼ì¸ì€ í—¤ë”ê°€ ì•„ë‹˜
    if not cleaned_line or len(cleaned_line) > 80:
        return None

    # ê° ì„¹ì…˜ íƒ€ì…ì˜ íŒ¨í„´ê³¼ ë§¤ì¹­
    for section_type, pattern_list in patterns.items():
        for pattern in pattern_list:
            if re.search(pattern, cleaned_line, re.IGNORECASE):
                return section_type

    return None


def is_section_header(line: str, file_type: FileTypeEnum) -> bool:
    """ë¼ì¸ì´ ì„¹ì…˜ í—¤ë”ì¸ì§€ í™•ì¸"""
    return detect_section_type(line, file_type) is not None


# ============================================================================
# ì„¹ì…˜ ê¸°ë°˜ ë¬¸ì„œ ë¶„í• 
# ============================================================================


@dataclass
class TextSection:
    """ë¶„í• ëœ ì„¹ì…˜ ì •ë³´"""

    section_type: SectionType
    content: str
    start_line: int
    end_line: int


def split_into_sections(
    text: str,
    file_type: FileTypeEnum,
) -> List[TextSection]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì„¹ì…˜ ë‹¨ìœ„ë¡œ ë¶„í• 

    Args:
        text: ì „ì²´ í…ìŠ¤íŠ¸
        file_type: íŒŒì¼ íƒ€ì…

    Returns:
        ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸
    """
    lines = text.split("\n")
    sections: List[TextSection] = []

    current_section_type = SectionType.UNKNOWN
    current_content_lines: List[str] = []
    current_start_line = 0

    for i, line in enumerate(lines):
        detected_type = detect_section_type(line, file_type)

        if detected_type is not None:
            # ì´ì „ ì„¹ì…˜ ì €ì¥ (ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°)
            if current_content_lines:
                content = "\n".join(current_content_lines).strip()
                if content:  # ë¹ˆ ì„¹ì…˜ì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
                    # ë‚´ìš© ê¸°ë°˜ ì„¹ì…˜ ì¶”ë¡  (unknownì¸ ê²½ìš°)
                    final_type = current_section_type
                    if final_type == SectionType.UNKNOWN:
                        inferred = infer_section_from_content(content)
                        if inferred:
                            final_type = inferred

                    sections.append(
                        TextSection(
                            section_type=final_type,
                            content=content,
                            start_line=current_start_line,
                            end_line=i - 1,
                        )
                    )

            # ìƒˆ ì„¹ì…˜ ì‹œì‘
            current_section_type = detected_type
            current_content_lines = [line]  # í—¤ë”ë„ í¬í•¨
            current_start_line = i
        else:
            current_content_lines.append(line)

    # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
    if current_content_lines:
        content = "\n".join(current_content_lines).strip()
        if content:
            final_type = current_section_type
            if final_type == SectionType.UNKNOWN:
                inferred = infer_section_from_content(content)
                if inferred:
                    final_type = inferred

            sections.append(
                TextSection(
                    section_type=final_type,
                    content=content,
                    start_line=current_start_line,
                    end_line=len(lines) - 1,
                )
            )

    return sections


# ============================================================================
# ì²­í‚¹ ì„¤ì •
# ============================================================================


@dataclass
class ChunkConfig:
    """ì²­í‚¹ ì„¤ì •"""

    chunk_size: int = 1000
    chunk_overlap: int = 200
    separators: List[str] = None

    def __post_init__(self):
        if self.separators is None:
            # ê¸°ë³¸ êµ¬ë¶„ì: ë‹¨ë½ > ë¬¸ì¥ > ë‹¨ì–´ ìˆœì„œë¡œ ë¶„í•  ì‹œë„
            self.separators = [
                "\n\n",  # ë¹ˆ ì¤„ (ë‹¨ë½ êµ¬ë¶„)
                "\n",  # ì¤„ë°”ê¿ˆ
                ". ",  # ë¬¸ì¥ ë
                "! ",
                "? ",
                "; ",
                ", ",  # ì ˆ êµ¬ë¶„
                " ",  # ë‹¨ì–´
                "",  # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ê¸€ì ë‹¨ìœ„
            ]


# íŒŒì¼ íƒ€ì…ë³„ ê¸°ë³¸ ì„¤ì •
DEFAULT_CHUNK_CONFIGS: Dict[FileTypeEnum, ChunkConfig] = {
    # ì´ë ¥ì„œ: ì„¹ì…˜ì´ ì§§ìœ¼ë¯€ë¡œ ì‘ì€ ì²­í¬
    FileTypeEnum.RESUME: ChunkConfig(
        chunk_size=800,
        chunk_overlap=150,
    ),
    # JD: ìƒì„¸ ì„¤ëª…ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í° ì²­í¬
    FileTypeEnum.JOB_DESCRIPTION: ChunkConfig(
        chunk_size=1000,
        chunk_overlap=200,
    ),
}


# ============================================================================
# ë©”ì¸ ì²­í‚¹ í•¨ìˆ˜
# ============================================================================


def create_text_splitter(config: ChunkConfig) -> RecursiveCharacterTextSplitter:
    """RecursiveCharacterTextSplitter ìƒì„±"""
    return RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        separators=config.separators,
        length_function=len,
        is_separator_regex=False,
    )


def chunk_document(
    document: Document,
    file_type: FileTypeEnum,
    config: Optional[ChunkConfig] = None,
    preserve_sections: bool = True,
) -> List[Document]:
    """
    LangChain Documentë¥¼ ì²­í¬ë¡œ ë¶„í• 

    Args:
        document: ì›ë³¸ Document
        file_type: íŒŒì¼ íƒ€ì…
        config: ì²­í‚¹ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        preserve_sections: ì„¹ì…˜ ê²½ê³„ ë³´ì¡´ ì—¬ë¶€

    Returns:
        ì²­í‚¹ëœ Document ë¦¬ìŠ¤íŠ¸
    """
    if config is None:
        config = DEFAULT_CHUNK_CONFIGS.get(
            file_type,
            ChunkConfig(),  # ê¸°ë³¸ê°’
        )

    text = document.page_content
    base_metadata = document.metadata.copy()

    if preserve_sections:
        # ì„¹ì…˜ ê¸°ë°˜ ì²­í‚¹
        return _chunk_with_sections(text, file_type, config, base_metadata)
    else:
        # ë‹¨ìˆœ ì²­í‚¹
        return _chunk_simple(text, config, base_metadata)


def _chunk_with_sections(
    text: str,
    file_type: FileTypeEnum,
    config: ChunkConfig,
    base_metadata: Dict[str, Any],
) -> List[Document]:
    """ì„¹ì…˜ì„ ë³´ì¡´í•˜ë©´ì„œ ì²­í‚¹"""
    sections = split_into_sections(text, file_type)
    splitter = create_text_splitter(config)

    chunks: List[Document] = []
    global_chunk_index = 0

    for section in sections:
        # ì„¹ì…˜ ë‚´ìš©ì´ ì²­í¬ í¬ê¸°ë³´ë‹¤ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if len(section.content) <= config.chunk_size:
            chunk_metadata = base_metadata.copy()
            chunk_metadata.update(
                {
                    "chunk_index": global_chunk_index,
                    "section_type": section.section_type.value,
                    "section_start_line": section.start_line,
                    "section_end_line": section.end_line,
                    "is_full_section": True,
                }
            )
            chunks.append(
                Document(
                    page_content=section.content,
                    metadata=chunk_metadata,
                )
            )
            global_chunk_index += 1
        else:
            # ì„¹ì…˜ ë‚´ìš©ì´ í¬ë©´ ì²­í‚¹
            section_chunks = splitter.split_text(section.content)

            for i, chunk_text in enumerate(section_chunks):
                chunk_metadata = base_metadata.copy()
                chunk_metadata.update(
                    {
                        "chunk_index": global_chunk_index,
                        "section_type": section.section_type.value,
                        "section_chunk_index": i,
                        "section_chunk_total": len(section_chunks),
                        "section_start_line": section.start_line,
                        "section_end_line": section.end_line,
                        "is_full_section": False,
                    }
                )
                chunks.append(
                    Document(
                        page_content=chunk_text,
                        metadata=chunk_metadata,
                    )
                )
                global_chunk_index += 1

    return chunks


def _chunk_simple(
    text: str,
    config: ChunkConfig,
    base_metadata: Dict[str, Any],
) -> List[Document]:
    """ë‹¨ìˆœ ì²­í‚¹ (ì„¹ì…˜ ë¬´ì‹œ)"""
    splitter = create_text_splitter(config)
    chunk_texts = splitter.split_text(text)

    chunks: List[Document] = []
    for i, chunk_text in enumerate(chunk_texts):
        # ë‚´ìš© ê¸°ë°˜ ì„¹ì…˜ ì¶”ë¡ 
        inferred_section = infer_section_from_content(chunk_text)
        section_type = inferred_section.value if inferred_section else SectionType.UNKNOWN.value

        chunk_metadata = base_metadata.copy()
        chunk_metadata.update(
            {
                "chunk_index": i,
                "section_type": section_type,
            }
        )
        chunks.append(
            Document(
                page_content=chunk_text,
                metadata=chunk_metadata,
            )
        )

    return chunks


# ============================================================================
# ê³ ê¸‰ ì²­í‚¹ í•¨ìˆ˜
# ============================================================================


def chunk_documents(
    documents: List[Document],
    file_type: FileTypeEnum,
    config: Optional[ChunkConfig] = None,
    preserve_sections: bool = True,
) -> List[Document]:
    """
    ì—¬ëŸ¬ Documentë¥¼ ì¼ê´„ ì²­í‚¹

    Args:
        documents: Document ë¦¬ìŠ¤íŠ¸
        file_type: íŒŒì¼ íƒ€ì…
        config: ì²­í‚¹ ì„¤ì •
        preserve_sections: ì„¹ì…˜ ê²½ê³„ ë³´ì¡´ ì—¬ë¶€

    Returns:
        ì²­í‚¹ëœ Document ë¦¬ìŠ¤íŠ¸
    """
    all_chunks: List[Document] = []

    for doc in documents:
        chunks = chunk_document(doc, file_type, config, preserve_sections)
        all_chunks.extend(chunks)

    # ì „ì²´ ì²­í¬ ì¸ë±ìŠ¤ ì¬í• ë‹¹
    for i, chunk in enumerate(all_chunks):
        chunk.metadata["global_chunk_index"] = i

    return all_chunks


def estimate_token_count(text: str, model: str = "text-embedding-3-small") -> int:
    """
    í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ì¶”ì • (OpenAI ëª¨ë¸ ê¸°ì¤€)

    ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±:
    - ì˜ì–´: ~4 ê¸€ì = 1 í† í°
    - í•œêµ­ì–´: ~2 ê¸€ì = 1 í† í°
    """
    # í•œê¸€ ë¹„ìœ¨ ê³„ì‚°
    korean_chars = len(re.findall(r"[ê°€-í£]", text))
    total_chars = len(text)

    if total_chars == 0:
        return 0

    korean_ratio = korean_chars / total_chars

    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ í† í° ìˆ˜ ì¶”ì •
    # í•œêµ­ì–´ ë¹„ìœ¨ì´ ë†’ì„ìˆ˜ë¡ ê¸€ìë‹¹ í† í° ìˆ˜ê°€ ë†’ìŒ
    chars_per_token = 4 - (korean_ratio * 2)  # 4 (ì˜ì–´) ~ 2 (í•œêµ­ì–´)

    return int(total_chars / chars_per_token)


def add_token_counts(chunks: List[Document]) -> List[Document]:
    """ì²­í¬ì— í† í° ìˆ˜ ì¶”ì •ì¹˜ ì¶”ê°€"""
    for chunk in chunks:
        chunk.metadata["estimated_tokens"] = estimate_token_count(chunk.page_content)
        chunk.metadata["char_count"] = len(chunk.page_content)

    return chunks


# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================


def get_chunk_summary(chunks: List[Document]) -> Dict[str, Any]:
    """ì²­í‚¹ ê²°ê³¼ ìš”ì•½"""
    if not chunks:
        return {"total_chunks": 0}

    total_chars = sum(len(c.page_content) for c in chunks)
    total_tokens = sum(c.metadata.get("estimated_tokens", 0) for c in chunks)

    section_counts: Dict[str, int] = {}
    for chunk in chunks:
        section = chunk.metadata.get("section_type", "unknown")
        section_counts[section] = section_counts.get(section, 0) + 1

    return {
        "total_chunks": len(chunks),
        "total_characters": total_chars,
        "estimated_tokens": total_tokens,
        "avg_chunk_size": total_chars // len(chunks) if chunks else 0,
        "section_distribution": section_counts,
    }


def merge_small_chunks(
    chunks: List[Document],
    min_chunk_size: int = 100,
) -> List[Document]:
    """
    ë„ˆë¬´ ì‘ì€ ì²­í¬ë¥¼ ì¸ì ‘ ì²­í¬ì™€ ë³‘í•©

    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        min_chunk_size: ìµœì†Œ ì²­í¬ í¬ê¸°

    Returns:
        ë³‘í•©ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    if len(chunks) <= 1:
        return chunks

    merged: List[Document] = []
    buffer: Optional[Document] = None

    for chunk in chunks:
        if buffer is None:
            buffer = chunk
        elif len(buffer.page_content) < min_chunk_size:
            # ë²„í¼ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ í˜„ì¬ ì²­í¬ì™€ ë³‘í•©
            buffer = Document(
                page_content=buffer.page_content + "\n\n" + chunk.page_content,
                metadata={
                    **buffer.metadata,
                    "merged": True,
                    "original_chunks": buffer.metadata.get("original_chunks", 1) + 1,
                },
            )
        else:
            merged.append(buffer)
            buffer = chunk

    # ë§ˆì§€ë§‰ ë²„í¼ ì²˜ë¦¬
    if buffer is not None:
        merged.append(buffer)

    return merged
